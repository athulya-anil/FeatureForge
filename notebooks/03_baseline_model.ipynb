{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Training - FeatureForge Phase 1\n",
    "\n",
    "This notebook trains the baseline CTR prediction model with 15-20 baseline features.\n",
    "\n",
    "## Goals:\n",
    "- Create baseline features\n",
    "- Train/val/test split\n",
    "- Train XGBoost model with class imbalance handling\n",
    "- Evaluate comprehensive metrics\n",
    "- **Establish baseline F1-score** (CONTROL group for A/B testing)\n",
    "- Analyze feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from src.config import Config\n",
    "from src.utils.logging_utils import setup_logging\n",
    "from src.utils.spark_utils import create_spark_session\n",
    "from src.data.loader import CriteoDataLoader\n",
    "from src.data.splitter import DataSplitter\n",
    "from src.features.feature_engine import FeatureEngine\n",
    "from src.models.trainer import XGBoostTrainer\n",
    "from src.models.evaluator import ModelEvaluator\n",
    "\n",
    "# Setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config('../config/config.yaml')\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging(level='INFO', log_file='../logs/baseline_model.log')\n",
    "\n",
    "logger.info(\"Starting baseline model training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = create_spark_session(\n",
    "    app_name=config['spark']['app_name'],\n",
    "    master=config['spark']['master'],\n",
    "    executor_memory=config['spark']['executor_memory'],\n",
    "    driver_memory=config['spark']['driver_memory']\n",
    ")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = CriteoDataLoader(spark, config)\n",
    "\n",
    "# Use sample for faster iteration (or full dataset if available)\n",
    "sample_path = config['data']['sample_path']\n",
    "print(f\"Loading data from: {sample_path}\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(sample_path) or len(os.listdir(sample_path)) == 0:\n",
    "    print(\"Sample not found. Loading raw data and creating sample...\")\n",
    "    raw_path = config['data']['raw_path']\n",
    "    df = loader.load_raw_data(raw_path)\n",
    "    df = loader.create_sample(df, config['data']['sample_size'], sample_path)\n",
    "else:\n",
    "    df = loader.load_parquet(sample_path)\n",
    "\n",
    "print(f\"Data loaded: {df.count():,} rows\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engine\n",
    "feature_engine = FeatureEngine(config)\n",
    "\n",
    "# Log feature summary\n",
    "feature_engine.log_feature_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline features\n",
    "print(\"Creating baseline features...\")\n",
    "df_features = feature_engine.create_baseline_features(df, is_training=True)\n",
    "\n",
    "print(f\"\\nFeatures created. Total columns: {len(df_features.columns)}\")\n",
    "print(f\"Feature columns: {df_features.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data splitter\n",
    "splitter = DataSplitter(config)\n",
    "\n",
    "# Split data\n",
    "train_df, val_df, test_df = splitter.split_data(df_features)\n",
    "\n",
    "print(f\"\\nTrain set: {train_df.count():,} rows\")\n",
    "print(f\"Validation set: {val_df.count():,} rows\")\n",
    "print(f\"Test set: {test_df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Training (Convert to Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature columns (exclude target and original categoricals)\n",
    "categorical_cols = config['features']['categorical_cols']\n",
    "target_col = config['features']['target_col']\n",
    "\n",
    "# Feature columns = all columns except target and original categoricals\n",
    "feature_cols = [c for c in df_features.columns if c != target_col and c not in categorical_cols]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "for i, col in enumerate(feature_cols[:20], 1):  # Show first 20\n",
    "    print(f\"  {i}. {col}\")\n",
    "if len(feature_cols) > 20:\n",
    "    print(f\"  ... and {len(feature_cols) - 20} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for XGBoost\n",
    "print(\"Converting to Pandas...\")\n",
    "\n",
    "# Select feature columns + target\n",
    "train_pd = train_df.select(feature_cols + [target_col]).toPandas()\n",
    "val_pd = val_df.select(feature_cols + [target_col]).toPandas()\n",
    "test_pd = test_df.select(feature_cols + [target_col]).toPandas()\n",
    "\n",
    "# Split features and target\n",
    "X_train = train_pd[feature_cols]\n",
    "y_train = train_pd[target_col]\n",
    "\n",
    "X_val = val_pd[feature_cols]\n",
    "y_val = val_pd[target_col]\n",
    "\n",
    "X_test = test_pd[feature_cols]\n",
    "y_test = test_pd[target_col]\n",
    "\n",
    "print(f\"\\nTraining data: {X_train.shape}\")\n",
    "print(f\"Validation data: {X_val.shape}\")\n",
    "print(f\"Test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = XGBoostTrainer(config)\n",
    "\n",
    "# Train model\n",
    "print(\"Training XGBoost model...\\n\")\n",
    "model = trainer.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on validation set\n",
    "y_val_proba = trainer.predict(X_val)\n",
    "y_val_pred = trainer.predict_binary(X_val, threshold=0.5)\n",
    "\n",
    "print(f\"Validation predictions: {len(y_val_pred):,}\")\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_proba = trainer.predict(X_test)\n",
    "y_test_pred = trainer.predict_binary(X_test, threshold=0.5)\n",
    "\n",
    "print(f\"Test predictions: {len(y_test_pred):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(output_dir='../results')\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "val_metrics = evaluator.evaluate(\n",
    "    y_val.values,\n",
    "    y_val_pred,\n",
    "    y_val_proba,\n",
    "    dataset_name=\"Validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "test_metrics = evaluator.evaluate(\n",
    "    y_test.values,\n",
    "    y_test_pred,\n",
    "    y_test_proba,\n",
    "    dataset_name=\"Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Baseline F1-Score (CONTROL Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract baseline F1-score\n",
    "baseline_f1 = test_metrics['f1']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ BASELINE MODEL - PHASE 1 RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä BASELINE F1-SCORE: {baseline_f1:.4f}\")\n",
    "print(f\"\\nThis F1-score serves as the CONTROL group for A/B testing.\")\n",
    "print(f\"In Phase 2, experimental features will be compared against this baseline.\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nOther Key Metrics:\")\n",
    "print(f\"  - AUC-ROC: {test_metrics['auc_roc']:.4f}\")\n",
    "print(f\"  - AUC-PR:  {test_metrics['auc_pr']:.4f}\")\n",
    "print(f\"  - Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  - Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "evaluator.plot_confusion_matrix(\n",
    "    test_metrics['confusion_matrix'],\n",
    "    save_path='../results/baseline_confusion_matrix.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "evaluator.plot_roc_curve(\n",
    "    y_test.values,\n",
    "    y_test_proba,\n",
    "    save_path='../results/baseline_roc_curve.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "evaluator.plot_precision_recall_curve(\n",
    "    y_test.values,\n",
    "    y_test_proba,\n",
    "    save_path='../results/baseline_pr_curve.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Distribution\n",
    "evaluator.plot_prediction_distribution(\n",
    "    y_test.values,\n",
    "    y_test_proba,\n",
    "    save_path='../results/baseline_prediction_dist.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance_df = trainer.get_feature_importance(importance_type='gain')\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "evaluator.plot_feature_importance(\n",
    "    importance_df,\n",
    "    top_n=20,\n",
    "    save_path='../results/baseline_feature_importance.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature importance to CSV\n",
    "importance_df.to_csv('../results/baseline_feature_importance.csv', index=False)\n",
    "print(\"Feature importance saved to: ../results/baseline_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "model_path = '../models/baseline_xgboost.model'\n",
    "trainer.save_model(model_path)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline metrics\n",
    "import json\n",
    "\n",
    "baseline_results = {\n",
    "    'phase': 1,\n",
    "    'model': 'baseline_xgboost',\n",
    "    'num_features': len(feature_cols),\n",
    "    'metrics': {\n",
    "        'f1': float(baseline_f1),\n",
    "        'auc_roc': float(test_metrics['auc_roc']),\n",
    "        'auc_pr': float(test_metrics['auc_pr']),\n",
    "        'precision': float(test_metrics['precision']),\n",
    "        'recall': float(test_metrics['recall']),\n",
    "        'accuracy': float(test_metrics['accuracy']),\n",
    "        'log_loss': float(test_metrics['log_loss'])\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = '../results/baseline_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "print(f\"Baseline results saved to: {results_path}\")\n",
    "print(\"\\nBaseline results:\")\n",
    "print(json.dumps(baseline_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ PHASE 1 COMPLETE - BASELINE MODEL ESTABLISHED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Key Results:\")\n",
    "print(f\"  - Baseline F1-Score: {baseline_f1:.4f}\")\n",
    "print(f\"  - Number of Features: {len(feature_cols)}\")\n",
    "print(f\"  - Training Samples: {len(X_train):,}\")\n",
    "print(f\"  - Test Samples: {len(X_test):,}\")\n",
    "print(f\"\\nüìÅ Outputs:\")\n",
    "print(f\"  - Model: {model_path}\")\n",
    "print(f\"  - Results: {results_path}\")\n",
    "print(f\"  - Visualizations: ../results/\")\n",
    "print(f\"\\nüöÄ Next Steps (Phase 2):\")\n",
    "print(f\"  1. Create 70+ experimental features\")\n",
    "print(f\"  2. Compare experimental vs baseline (A/B testing)\")\n",
    "print(f\"  3. Statistical significance testing\")\n",
    "print(f\"  4. Feature selection and optimization\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"\\nSpark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
